# -*- coding: utf-8 -*-

import subprocess
import datetime
import platform
import random
import time
from datetime import timedelta
from time import gmtime
from time import sleep
from time import strftime
import chromedriver_autoinstaller
import pyshorteners
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.common.exceptions import NoSuchElementException
from selenium.webdriver import ActionChains
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
import os.path
from bs4 import BeautifulSoup
import time
import requests
import re
import os
import json
import hashlib
import hmac
from time import gmtime, strftime
from pprint import pprint as pp
from urllib.parse import urljoin
from fake_useragent import UserAgent
from urllib import parse
from urllib.parse import urlparse, parse_qs

osName = platform.system()  # window ì¸ì§€ mac ì¸ì§€ ì•Œì•„ë‚´ê¸° ìœ„í•œ
# print(osName)

C_END = "\033[0m"
C_BOLD = "\033[1m"
C_INVERSE = "\033[7m"
C_BLACK = "\033[30m"
C_RED = "\033[31m"
C_GREEN = "\033[32m"
C_YELLOW = "\033[33m"
C_BLUE = "\033[34m"
C_PURPLE = "\033[35m"
C_CYAN = "\033[36m"
C_WHITE = "\033[37m"
C_BGBLACK = "\033[40m"
C_BGRED = "\033[41m"
C_BGGREEN = "\033[42m"
C_BGYELLOW = "\033[43m"
C_BGBLUE = "\033[44m"
C_BGPURPLE = "\033[45m"
C_BGCYAN = "\033[46m"
C_BGWHITE = "\033[47m"

# [ì‚¬ìš©ì ì…ë ¥ ì •ë³´] ======================================================================================================== START

# ì¿ íŒ¡ íŒŒíŠ¸ë„ˆ API access keyì™€ secret key
coopang_access_key = ""  # access_key ë¥¼ ì…ë ¥í•˜ì„¸ìš”!
coopang_secret_key = ""  # secret_key ë¥¼ ì…ë ¥í•˜ì„¸ìš”!

# ë§í¬ ë³€í™˜ì„ í•  ìˆ˜ ì—†ëŠ” ê²½ìš°ë¥¼ ìœ„í•´ fake ì¿ íŒ¡ ë§í¬ë¥¼ ë§Œë“¤ì–´ì¤€ë‹¤.
fake_coopang_link = 'https://link.coupang.com/a/bawvXk'  # ê°„í¸ ë§í¬ë¥¼ í†µí•´ ë§Œë“  ë³¸ì¸ ë§í¬

# ì¿ íŒ¡ íŒŒíŠ¸ë„ˆìŠ¤ ë³¸ì¸ AF ì½”ë“œ
AFCODE = 'AF4962993'

# ë³¸ì¸ë“¤ì´ ë§Œë“  ì¿ íŒ¡ ì±„ë„ ì•„ì´ë”” defaultë¥¼ ì“°ê³  ìˆë‹¤ë©´ ë¹ˆê³µë€
CHANNELID = 'github1moneydory'

# pause time ì •ë³´
PAUSE_TIME = 0.5
LOADING_WAIT_TIME = 3
LOGIN_WAIT_TIME = 180
GENERAL_REQUEST_WAIT_TIME = 0.3  # ì¼ë°˜ì ì¸ requestsì˜ ì†ë„

# # fake-useragent / user-agent
# # https://domdom.tistory.com/329
# # ua = UserAgent(use_cache_server=True)
# ua = UserAgent(verify_ssl=False)
# user_agent = ua.random
# print(f'User-Agent : {user_agent}')
fixed_user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Whale/3.19.166.16 Safari/537.36'

# itemscout category ë° íŒŒë¼ë¯¸í„° ì„ íƒ
# 1ì°¨ë¶„ë¥˜: ì„ íƒì•ˆí•¨,
#        íŒ¨ì…˜ì˜ë¥˜(1)(ì—¬ì„±ì˜ë¥˜, ì—¬ì„±ì–¸ë”ì›¨ì–´/ì ì˜·, ë‚¨ì„±ì˜ë¥˜, ë‚¨ì„±ì–¸ë”ì›¨ì–´/ì ì˜·),
#        íŒ¨ì…˜ì¡í™”(2)(ì–‘ë§, ì—¬ì„±ì‹ ë°œ, ë‚¨ì„±ì‹ ë°œ, ì‹ ë°œìš©í’ˆ, ì—¬ì„±ê°€ë°©, ë‚¨ì„±ê°€ë°©, ì—¬í–‰ìš©ê°€ë°©/ì†Œí’ˆ, ì§€ê°‘, ë²¨íŠ¸, ëª¨ì, ì¥ê°‘, ì„ ê¸€ë¼ìŠ¤/ì•ˆê²½íƒœ, í—¤ì–´ì•¡ì„¸ì„œë¦¬, íŒ¨ì…˜ì†Œí’ˆ, ì‹œê³„, ìˆœê¸ˆ, ì£¼ì–¼ë¦¬),
#        í™”ì¥í’ˆ/ë¯¸ìš©(3)(ìŠ¤í‚¨ì¼€ì–´, ì„ ì¼€ì–´, í´ë Œì§•, ë§ˆìŠ¤í¬/íŒ©, ë² ì´ìŠ¤ë©”ì´í¬ì—…, ìƒ‰ì¡°ë©”ì´í¬ì—…, ë„¤ì¼ì¼€ì–´, ë°”ë””ì¼€ì–´, í—¤ì–´ì¼€ì–´, í—¤ì–´ìŠ¤íƒ€ì¼ë§, í–¥ìˆ˜, ë·°í‹°ì†Œí’ˆ, ë‚¨ì„±í™”ì¥í’ˆ),
#        ë””ì´í„¸/ê°€ì „(4)(í•™ìŠµê¸°ê¸°, ê²Œì„ê¸°/íƒ€ì´í‹€, PC, PCì•¡ì„¸ì„œë¦¬, ë…¸íŠ¸ë¶ì•¡ì„¸ì„œë¦¬, íƒœë¸”ë¦¿PCì•¡ì„¸ì„œë¦¬, ëª¨ë‹ˆí„°ì£¼ë³€ê¸°ê¸°, ì£¼ë³€ê¸°ê¸°, ë©€í‹°ë¯¸ë””ì–´ì¥ë¹„, ì €ì¥ì¥ì¹˜, PCë¶€í’ˆ, ë„¤íŠ¸ì›Œí¬ì¥ë¹„, ì†Œí”„íŠ¸ì›¨ì–´, ë…¸íŠ¸ë¶, íƒœë¸”ë¦¿PC, ëª¨ë‹ˆí„°, íœ´ëŒ€í°, íœ´ëŒ€í°ì•…ì„¸ì„œë¦¬, ì¹´ë©”ë¼/ìº ì½”ë”ìš©í’ˆ, ê´‘í•™ê¸°ê¸°/ìš©í’ˆ, ì˜ìƒê°€ì „, ìŒí–¥ê°€ì „, ì´ë¯¸ìš©ê°€ì „, ê³„ì ˆê°€ì „, ì£¼ë°©ê°€ì „, ìë™ì°¨ê¸°ê¸°),
#        ê°€êµ¬/ì¸í…Œë¦¬ì–´(5)(ì¹¨ì‹¤ê°€êµ¬, ê±°ì‹¤ê°€êµ¬, ì£¼ë°©ê°€êµ¬, ìˆ˜ë‚©ê°€êµ¬, ì•„ë™/ì£¼ë‹ˆì–´ê°€êµ¬, ì„œì¬/ì‚¬ë¬´ìš©ê°€êµ¬, ì•„ì›ƒë„ì–´ê°€êµ¬, DIYìì¬/ìš©í’ˆ, ì¸í…Œë¦¬ì–´ì†Œí’ˆ, ì¹¨êµ¬ë‹¨í’ˆ, ì¹¨êµ¬ì„¸íŠ¸, ì†œë¥˜, ì¹´í˜íŠ¸/ëŸ¬ê·¸, ì»¤í°/ë¸”ë¼ì¸ë“œ, ìˆ˜ì˜ˆ, í™ˆë°ì½”),
#        ì¶œì‚°/ìœ¡ì•„(6)(ë¶„ìœ , ê¸°ì €ê·€, ë¬¼í‹°ìŠˆ, ì´ìœ ì‹, ì•„ê¸°ê°„ì‹, ìˆ˜ìœ ìš©í’ˆ, ìœ ëª¨ì°¨, ì¹´ì‹œíŠ¸, ì™¸ì¶œìš©í’ˆ, ëª©ìš©ìš©í’ˆ, ìŠ¤í‚¨/ë°”ë””ìš©í’ˆ, ìœ„ìƒ/ê±´ê°•ìš©í’ˆ, êµ¬ê°•ì² ê²°ìš©í’ˆ, ìœ ì•„ì„¸ì œ, ì†Œë…/ì‚´ê· ìš©í’ˆ, ì•ˆì „ìš©í’ˆ, ìœ ì•„ê°€êµ¬, ì´ìœ ì‹ìš©í’ˆ, ì„ë¶€ë³µ, ì„ì‚°ë¶€ìš©í’ˆ, ìœ ì•„ì¹¨êµ¬, ì¶œì‚°/ëŒê¸°ë…í’ˆ, ì‹ ìƒì•„ì˜ë¥˜, ìœ ì•„ë™ì˜ë¥˜, ìœ ì•„ë™ì¡í™”, ìˆ˜ì˜ë³µ/ìš©í’ˆ, ìœ ì•„ë°œìœ¡ìš©í’ˆ, ì™„êµ¬/ë§¤íŠ¸, ì¸í˜•, êµêµ¬, ìœ ì•„ë™ ì£¼ì–¼ë¦¬, ìœ ì•„ë™ì–¸ë”ì›¨ì–´/ì ì˜·),
#        ì‹í’ˆ(7)(ê±´ê°•ì‹í’ˆ, ë‹¤ì´ì–´íŠ¸ì‹í’ˆ, ëƒ‰ë™/ê°„í¸ì¡°ë¦¬ì‹í’ˆ, ì¶•ì‚°ë¬¼, ë°˜ì°¬, ê¹€ì¹˜, ìŒë£Œ, ê³¼ì/ë² ì´ì»¤ë¦¬, ìœ ê°€ê³µí’ˆ, ìˆ˜ì‚°ë¬¼, ë†ì‚°ë¬¼, ë°€í‚¤íŠ¸, ê°€ë£¨/ë¶„ë§ë¥˜, ë¼ë©´/ë©´ë¥˜, ì†ŒìŠ¤/ë“œë ˆì‹±, ì‹ìš©ìœ /ì˜¤ì¼, ì¥ë¥˜, ì¼/ì‹œëŸ½, ì œê³¼/ì œë¹µì¬ë£Œ, ì¡°ë¯¸ë£Œ, í†µì¡°ë¦¼/ìº”, ì£¼ë¥˜),
#        ìŠ¤í¬ì¸ /ë ˆì €(8)(ë§ˆë¼í†¤ìš©í’ˆ, ë‹¹êµ¬ìš©í’ˆ, ê¸°íƒ€ìŠ¤í¬ì¸ ìš©í’ˆ, ë“±ì‚°, ìº í•‘, ê³¨í”„, í—¬ìŠ¤, ìš”ê°€/í•„ë¼í…ŒìŠ¤, ì¸ë¼ì¸ìŠ¤ì¼€ì´íŠ¸, ìŠ¤ì¼€ì´íŠ¸/ë³´ë“œ/ë¡¤ëŸ¬, ì˜¤í† ë°”ì´/ìŠ¤ì¿ í„°, ì¶•êµ¬, ì•¼êµ¬, ë†êµ¬, ë°°êµ¬, íƒêµ¬, ë°°ë“œë¯¼í„´, í…Œë‹ˆìŠ¤, ìŠ¤ì¿¼ì‹œ, ì¡±êµ¬, ë³¼ë§, ìŠ¤í‚¨ìŠ¤ì¿ ë²„, ê²€ë„, ëŒ„ìŠ¤, ê¶Œíˆ¬, ë³´í˜¸ìš©í’ˆ, ìˆ˜ë ¨ìš©í’ˆ, ìŠ¤í¬ì¸ ì•¡ì„¸ì„œë¦¬, ìì „ê±°, ìŠ¤í‚¤/ë³´ë“œ, ë‚šì‹œ, ìˆ˜ì˜),
#        ìƒí™œ/ê±´ê°•(9)(í™”ë°©ìš©í’ˆ, ìë™ì°¨ìš©í’ˆ, ìˆ˜ì§‘í’ˆ, ê´€ìƒì–´ìš©í’ˆ, ìŒë°˜, DVD, ì¢…êµ, ì£¼ë°©ìš©í’ˆ, ì„¸íƒìš©í’ˆ, ê±´ê°•ì¸¡ì •ìš©í’ˆ, ê±´ê°•ê´€ë¦¬ìš©í’ˆ, ë‹¹ë‡¨ê´€ë¦¬ìš©í’ˆ, ì˜ë£Œìš©í’ˆ, ì‹¤ë²„ìš©, ì¬í™œìš´ë™ìš©í’ˆ, ë¬¼ë¦¬ì¹˜ë£Œ/ì €ì£¼íŒŒìš©í’ˆ, ì¢Œìš•/ì¢Œí›ˆìš©í’ˆ, ëƒ‰ì˜¨/ì°œì§ˆìš©í’ˆ, êµ¬ê°•ìœ„ìƒìš©í’ˆ, ëˆˆê±´ê°•ìš©í’ˆ, ë°œê±´ê°•ìš©í’ˆ, ì•ˆë§ˆìš©í’ˆ, ìˆ˜ë‚©/ì •ë¦¬ìš©í’ˆ, ì²­ì†Œìš©í’ˆ, ìƒí™œìš©ì¶¤, ì›ì˜ˆ/ì‹ë¬¼, ì •ì›/ì›ì˜ˆìš©í’ˆ, ë¸”ë£¨ë ˆì´, ë°˜ë ¤ë™ë¬¼, ì•…ê¸°, ìš•ì‹¤ìš©í’ˆ, ë¬¸êµ¬/ì‚¬ë¬´ìš©í’ˆ, ê³µêµ¬),
#        ì—¬ê°€/ìƒí™œí¸ì˜(10)(ì›ë°ì´í´ë˜ìŠ¤, êµ­ë‚´ì—¬í–‰/ì²´í—˜, í•´ì™¸ì—¬í–‰, ë Œí„°ì¹´, ìƒí™œí¸ì˜, ì˜ˆì²´ëŠ¥ë ˆìŠ¨, ìê¸°ê³„ë°œ/ì·¨ë¯¸ë ˆìŠ¨, í™ˆì¼€ì–´ì„œë¹„ìŠ¤),
#        ë©´ì„¸ì (11)(í™”ì¥í’ˆ, í–¥ìˆ˜, ì‹œê³„/ê¸°í”„íŠ¸, ì£¼ì–¼ë¦¬, íŒ¨ì…˜/ì¡í™”, ì „ìì œí’ˆ, ì‹í’ˆ/ê±´ê°•),
#        ë„ì„œ(45830)(ì†Œì„¤, ì‹œ/ì—ì„¸ì´, ì¸ë¬¸, ê°€ì •/ìš”ë¦¬, ê²½ì œ/ê²½ì˜, ìê¸°ê³„ë°œ, ì‚¬íšŒ/ì •ì¹˜, ì—­ì‚¬, ì¢…êµ, ì˜ˆìˆ /ëŒ€ì¤‘ë¬¸í™”, êµ­ì–´/ì™¸êµ­ì–´, ìì—°/ê³¼í•™, ìˆ˜í—˜ì„œ/ìê²©ì¦, ì—¬í–‰, ì»´í“¨í„°/IT, ì¡ì§€, ì²­ì†Œë…„, ìœ ì•„, ì–´ë¦°ì´, ë§Œí™”, ì™¸êµ­ë„ì„œ, ê±´ê°€/ì·¨ë¯¸, ì¤‘í•™êµì°¸ê³ ì„œ, ê³ ë“±í•™êµì°¸ê³ ì„œ, ì´ˆë“±í•™êµì°¸ê³ ì„œ, ì¤‘ê³ ë„ì„œ)
# ê¸°ê°„: ìµœê·¼ 30ì¼, ê³¼ê±°ì„ íƒ(ë‚ ì§œì„ íƒ) (duration 30d ë˜ëŠ” ë‚ ì§œ ì„¤ì •, duration: 2023-03,2023-04 3ì›”ë¶€í„° 4ì›”)
# ì„±ë³„: ì „ì²´, ë‚¨ì„±, ì—¬ì„± (sample. ë‚¨ì„±ê³¼ ì—¬ì„±ì´ë©´ f,m)
# ì—°ë ¹ëŒ€: 10, 20, 30, 40, 50, 60 (sample. 20,60 ì´ë©´ 20ëŒ€ë¶€í„° 60ëŒ€ê¹Œì§€)

# ì•„ì´í…œìŠ¤ì¹´ìš°íŠ¸ ì¹´í…Œê³ ë¦¬ (1ì°¨ ë¶„ë¥˜ë§Œ ìˆì„ë•Œ)
random_itemscout_category_lists = ['íŒ¨ì…˜ì˜ë¥˜', 'íŒ¨ì…˜ì¡í™”', 'í™”ì¥í’ˆ/ë¯¸ìš©', 'ë””ì´í„¸/ê°€ì „', 'ê°€êµ¬/ì¸í…Œë¦¬ì–´',
                                   'ì¶œì‚°/ìœ¡ì•„', 'ì‹í’ˆ', 'ìŠ¤í¬ì¸ /ë ˆì €', 'ìƒí™œ/ê±´ê°•', 'ì—¬ê°€/ìƒí™œí¸ì˜', 'ë©´ì„¸ì ', 'ë„ì„œ']
random_itemscout_cid_lists = ['1', '2', '3', '4', '5',
                              '6', '7', '8', '9', '10', '11', '45830']

# ì¶”ì¶œí•˜ê³ ì í•˜ëŠ” ê° ì¹´í…Œê³ ë¦¬ë³„ keyword ìˆ˜ (ë„¤ì´ë²„ max:500, ì•„ì´í…œìŠ¤ì¹´ìš°íŠ¸ max: "ì•„ì´í…œë°œêµ´"ì„ í†µí•œ 500)
wanted_item_num = 500

# github pages ë¥¼ ìœ„í•œ post md íŒŒì¼ì´ ì €ì¥ë  ìœ„ì¹˜
post_md_location = r"C:\Users\sundo\Desktop\workspace\moneydory.github.io\_posts"

# [ì‚¬ìš©ì ì…ë ¥ ì •ë³´] ======================================================================================================== END

# [ì‹œìŠ¤í…œ ê³µí†µ ì…ë ¥ ì •ë³´] ======================================================================================================== START

# partner product info
product_name_lists = []  # ìƒí’ˆëª…
product_discount_rate_lists = []  # í• ì¸ë¥ ê³¼ ì›ë˜ê°€ê²©
product_price_lists = []  # ìƒí’ˆê°€ê²©
product_arrival_time_lists = []  # ë„ì°©ì˜ˆì •
product_rating_star_lists = []  # star í‰ê°€: ex.3.5
product_review_lists = []  # ìƒí’ˆë¦¬ë·° ìˆ˜
product_link_lists = []  # ìƒí’ˆ êµ¬ë§¤ ë§í¬
product_image_lists = []  # ìƒí’ˆ ì´ë¯¸ì§€
product_short_url_lists = []  # ì¿ íŒ¡ ìˆ ë§í¬ ë¦¬ìŠ¤íŠ¸
shorten_url_lists = []  # ìˆë§í¬ ìƒì„±ê¸°ë¡œ ë§Œë“  ë§í¬ ë¦¬ìŠ¤íŠ¸

##-------------------------------------------------------------------------------------------------------

# itemscout ì—ì„œ ì•„ì´í…œë“¤ ëŒ€í•œ ì •ë³´ ë¦¬ìŠ¤íŠ¸
keyword_id_lists = []  # keyword ì•„ì´ë””
rank_lists = []  # ë­í‚¹
keyword_name_lists = []  # í‚¤ì›Œë“œ
no_of_search_total_lists = []  # ê²€ìƒ‰ìˆ˜
prdCnt_lists = []  # ìƒí’ˆìˆ˜

# itemscout ì—ì„œ ê° keyword ì— ë”°ë¥¸ ì „ë°˜ì ì¸ ë¶„ì„ì„ ìœ„í•œ ë¦¬ìŠ¤íŠ¸
competitionIntensity_lists = []  # ê²½ìŸê°•ë„ ìˆ˜ì¹˜
competitionIntensityDesc_lists = []  # ê²½ìŸê°•ë„ ë“±ê¸‰
adClickRateTotal_lists = []  # ê´‘ê³ í´ë¦­ë¥  ìˆ˜ì¹˜
adClickRateTotalDesc_lists = []  # ê´‘ê³ í´ë¦­ë¥  ë“±ê¸‰

# itemscout ì—ì„œ ê° keyword ì— ë”°ë¥¸ ë¸”ë¡œê·¸ ì¹´í˜ ë¶„ì„ ë¦¬ìŠ¤íŠ¸
blogCompetitionRatio_lists = []  # ë¸”ë¡œê·¸ ê²½ìŸê°•ë„ ìˆ˜ì¹˜
blogCompetitionDesc_lists = []  # ë¸”ë¡œê·¸ ê²½ìŸê°•ë„ ë“±ê¸‰
cafeCompetitionRatio = []  # ì¹´í˜ ê²½ìŸê°•ë„ ìˆ˜ì¹˜
cafeCompetitionDesc_lists = []  # ì¹´í˜ ê²½ìŸê°•ë„ ë“±ê¸‰

# itemscout ì—ì„œ ê° keyword ì— ë”°ë¥¸ ì¿ íŒ¡ ë¶„ì„ ë¦¬ìŠ¤íŠ¸
coupangCompetitionRatio_lists = []  # ì¿ íŒ¡ ê²½ìŸê°•ë„ ìˆ˜ì¹˜
coupangCompetitionDesc_lists = []  # ì¿ íŒ¡ ê²½ìŸê°•ë„ ë“±ê¸‰

# ëœë¤ ìƒì„± ë¦¬ìŠ¤íŠ¸
itemscout_cid_category_lists = []
itemscout_duration_lists = []
itemscout_age_lists = []
itemscout_gender_list = []

# ëœë¤ ìˆ˜ ìƒì„± ì´ë¯¸ ë‚˜ì˜¨ ëœë¤ ë²ˆí˜¸ëŠ” ì•ˆë‚˜ì˜¤ë„ë¡ (ê²¹ì¹˜ì§€ ì•ŠëŠ” ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•˜ê¸° ìœ„í•´)
random_number_list = []


# [ì‹œìŠ¤í…œ ê³µí†µ ì…ë ¥ ì •ë³´] ======================================================================================================== END


def init_driver():
    # try :
    #     shutil.rmtree(r"C:\chrometemp")  #ì¿ í‚¤ / ìºì‰¬íŒŒì¼ ì‚­ì œ(ìºì‰¬íŒŒì¼ ì‚­ì œì‹œ ë¡œê·¸ì¸ ì •ë³´ ì‚¬ë¼ì§)
    # except FileNotFoundError :
    #     pass

    # if osName not in "Windows":
    #     try:
    #         subprocess.Popen([
    #             '/Applications/Google\ Chrome.app/Contents/MacOS/Google\ Chrome --remote-debugging-port=9222 --user-data-dir="~/Desktop/crawling/chromeTemp22"'],
    #             shell=True, stdout=subprocess.PIPE)  # ë””ë²„ê±° í¬ë¡¬ êµ¬ë™
    #     except FileNotFoundError:
    #         subprocess.Popen([
    #             '/Applications/Google\ Chrome.app/Contents/MacOS/Google\ Chrome --remote-debugging-port=9222 --user-data-dir="~/Desktop/crawling/chromeTemp22"'],
    #             shell=True, stdout=subprocess.PIPE)
    # else:
    #     try:
    #         subprocess.Popen(r'C:\Program Files\Google\Chrome\Application\chrome.exe --remote-debugging-port=9222 '
    #                          r'--user-data-dir="C:\chrometemp22"')  # ë””ë²„ê±° í¬ë¡¬ êµ¬ë™
    #     except FileNotFoundError:
    #         subprocess.Popen(
    #             r'C:\Program Files\Google\Chrome\Application\chrome.exe --remote-debugging-port=9222 '
    #             r'--user-data-dir="C:\chrometemp22"')

    chrome_options = Options()
    chrome_options.add_experimental_option("debuggerAddress", "127.0.0.1:9222")

    # if osName not in "Windows":
    #     chrome_ver = chromedriver_autoinstaller.get_chrome_version().split('.')[0]  # í¬ë¡¬ë“œë¼ì´ë²„ ë²„ì „ í™•ì¸
    #     try:
    #         driver = webdriver.Chrome(f'./{chrome_ver}/chromedriver', options=chrome_options)
    #     except:
    #         chromedriver_autoinstaller.install(True)
    #         driver = webdriver.Chrome(f'./{chrome_ver}/chromedriver', options=chrome_options)
    # else:
    #     chrome_ver = chromedriver_autoinstaller.get_chrome_version().split('.')[0]  # í¬ë¡¬ë“œë¼ì´ë²„ ë²„ì „ í™•ì¸
    #     try:
    #         driver = webdriver.Chrome(f'./{chrome_ver}/chromedriver.exe', options=chrome_options)
    #     except:
    #         chromedriver_autoinstaller.install(True)
    #         driver = webdriver.Chrome(f'./{chrome_ver}/chromedriver.exe', options=chrome_options)

    # driver = webdriver.Chrome('./chromedriver.exe', options=chrome_options)
    driver = webdriver.Chrome(f'/Users/taesub/Desktop/crawling/chromedriver', options=chrome_options)
    driver.implicitly_wait(LOADING_WAIT_TIME)
    return driver


def itemscout_login(driver):
    driver.get('https://login.aliexpress.com/')
    sleep(LOADING_WAIT_TIME)

    print(f'\n{C_BOLD}{C_RED}{C_BGBLACK}[ì£¼ì˜: 3ë¶„ì•ˆì— ë¡œê·¸ì¸ì„ ì™„ë£Œí•´ì£¼ì„¸ìš”!!!]{C_END}')
    pbar = tqdm(total=LOGIN_WAIT_TIME)
    for x in range(LOGIN_WAIT_TIME):
        sleep(1)
        try:
            driver.find_element(By.CLASS_NAME, 'ng-account-icon')
            break
        except:
            pass
        pbar.update(1)
    pbar.close()


def get_cookies_session(driver, url):
    driver.get(url)
    sleep(LOADING_WAIT_TIME)

    _cookies = driver.get_cookies()
    cookie_dict = {}
    for cookie in _cookies:
        cookie_dict[cookie['name']] = cookie['value']
        print(f"{cookie['name']} | {cookie['value']}")
    # print(cookie_dict)

    _session = requests.Session()
    headers = {
        'User-Agent': fixed_user_agent,
    }
    # print(f'\n{_session.headers}')
    # print(f'\n{_session.cookies}')

    _session.headers.update(headers)  # User-Agent ë³€ê²½
    print(f'\n{_session.headers}')

    _session.cookies.update(cookie_dict)  # ì‘ë‹µë°›ì€ cookiesë¡œ  ë³€ê²½
    print(f'\n{_session.cookies}')

    # # ì…€ë ˆë‹ˆì›€ ì›¹ ë“œë¼ì´ë²„ë¥¼ ì¢…ë£Œ(drivet.quit())
    # print(f'\nì„¸ì…˜ ì •ë³´ë¥¼ ì–»ì–´ì™”ìŠµë‹ˆë‹¤. ì…€ë ˆë‹ˆì›€ ì›¹ ë“œë¦¬ì´ë²„ë¥¼ ì¢…ë£Œí•˜ê² ìŠµë‹ˆë‹¤.')
    #
    # if url == 'https://ch.kakao.com/channels/@issuehouse':  # ì´ìŠˆ í•˜ìš°ìŠ¤ ì±„ë„ì— ëŒ€í•œ í˜ì´ì§€ì¼ë•Œë§Œ close
    #     driver.close()
    #     driver.quit()

    return _session


def recursive_random_int_no_again(_count):
    if _count == 0:  # ì¹´ìš´íŠ¸ê°€ 0ì´ ë˜ë©´ ì‘ë™ ì¢…ë£Œ
        print(random_number_list)
        return
    a = random.randint(1, 12)  # ì•„ì‰½ì§€ë§Œ ìˆ˜ë™ìœ¼ë¡œ _count ìˆ˜ë¥¼ ë„£ì–´ì¤Œ
    while a in random_number_list:
        a = random.randint(1, 12)  # ì•„ì‰½ì§€ë§Œ ìˆ˜ë™ìœ¼ë¡œ _count ìˆ˜ë¥¼ ë„£ì–´ì¤Œ
    random_number_list.append(a)
    _count -= 1  # ì¹´ìš´íŠ¸ë¥¼ - 1í•˜ê³ , í•¨ìˆ˜ë¥¼ ë‹¤ì‹œ í˜¸ì¶œ í•œë‹¤.
    recursive_random_int_no_again(_count)


def get_items_for_itemscout(cid, category, duration, ages, gender):
    # itemscout ëœë¤ ìƒì„± ë¦¬ìŠ¤íŠ¸
    global itemscout_cid_category_lists
    itemscout_cid_category_lists = []
    global itemscout_duration_lists
    itemscout_duration_lists = []
    global itemscout_age_lists
    itemscout_age_lists = []
    global itemscout_gender_list
    itemscout_gender_list = []

    global keyword_id_lists
    keyword_id_lists = []  # keyword ì•„ì´ë””
    global rank_lists
    rank_lists = []  # ë­í‚¹
    global keyword_name_lists
    keyword_name_lists = []  # í‚¤ì›Œë“œ
    global no_of_search_total_lists
    no_of_search_total_lists = []  # ê²€ìƒ‰ìˆ˜
    global prdCnt_lists
    prdCnt_lists = []  # ìƒí’ˆìˆ˜

    headers = {
        'accept': 'application/json, text/plain, */*',
        'content-type': 'application/x-www-form-urlencoded',
        'origin': 'https://itemscout.io',
        'referer': 'https://itemscout.io/',
        'user-agent': fixed_user_agent,
    }

    data = {
        'duration': parse.quote(duration, encoding="utf-8"),  # url encoding í›„ ë„£ì–´ì¤Œ
        'genders': parse.quote(gender, encoding="utf-8"),
        'ages': parse.quote(ages, encoding="utf-8")
    }

    response = requests.post(f'https://api.itemscout.io/api/category/{cid}/data', headers=headers, data=data).json()
    # pp(response)

    for idx, item in enumerate(response['data']['data'].items()):
        if idx == wanted_item_num:
            break
        # print(f'{item}')
        # print(f"keywordID(keyword ID): {item[0]}")  # keyword ì•„ì´ë””
        # print(f"rank(ë­í‚¹): {item[1]['rank']}")  # ë­í‚¹
        # print(f"keyword(í‚¤ì›Œë“œ): {item[1]['keyword']}")  # í‚¤ì›Œë“œ
        # print(f"total(ê²€ìƒ‰ìˆ˜): {item[1]['monthly']['total']}")  # ê²€ìƒ‰ìˆ˜
        # print(f"prdCnt(ìƒí’ˆìˆ˜): {item[1]['prdCnt']}")  # ìƒí’ˆìˆ˜
        print(
            f"{idx + 1}. ID {item[0]} | keyword {item[1]['keyword']} | ë­í‚¹ {item[1]['rank']} | ê²€ìƒ‰ìˆ˜ {item[1]['monthly']['total']} | ìƒí’ˆìˆ˜ {item[1]['prdCnt']}")
        itemscout_cid_category_lists.append(category)
        itemscout_duration_lists.append(duration)
        itemscout_age_lists.append(ages)
        itemscout_gender_list.append(gender)
        keyword_id_lists.append(item[0])
        rank_lists.append(item[1]['rank'])
        keyword_name_lists.append(item[1]['keyword'])
        no_of_search_total_lists.append(item[1]['monthly']['total'])
        prdCnt_lists.append(item[1]['prdCnt'])


def get_keyword_stats_for_itemscout():
    global competitionIntensity_lists
    competitionIntensity_lists = []  # ê²½ìŸê°•ë„ ìˆ˜ì¹˜
    global competitionIntensityDesc_lists
    competitionIntensityDesc_lists = []  # ê²½ìŸê°•ë„ ë“±ê¸‰
    global adClickRateTotal_lists
    adClickRateTotal_lists = []  # ê´‘ê³ í´ë¦­ë¥  ìˆ˜ì¹˜
    global adClickRateTotalDesc_lists
    adClickRateTotalDesc_lists = []  # ê´‘ê³ í´ë¦­ë¥  ë“±ê¸‰

    headers = {
        'accept': 'application/json, text/plain, */*',
        'origin': 'https://itemscout.io',
        'referer': 'https://itemscout.io/',
        'user-agent': fixed_user_agent,
    }

    for idx, keyword_id in enumerate(keyword_id_lists):
        response = requests.get(f'https://api.itemscout.io/api/v2/keyword/stats/{keyword_id}', headers=headers).json()
        # pp(response)
        # print(response['data']['competitionIntensity'])  # ê²½ìŸê°•ë„ ìˆ˜ì¹˜
        # print(response['data']['competitionIntensityDesc'])  # ê²½ìŸê°•ë„ ë“±ê¸‰
        # print(response['data']['adClickRateStats']['adClickRateTotal'])  # ê´‘ê³ í´ë¦­ë¥  ìˆ˜ì¹˜
        # print(response['data']['adClickRateStats']['adClickRateTotalDesc'])  # ê´‘ê³ í´ë¦­ë¥  ë“±ê¸‰
        print(
            f"{idx + 1}. ê²½ìŸê°•ë„({response['data']['competitionIntensityDesc']}) | ê´‘ê³ í´ë¦­ë¥ ({response['data']['adClickRateStats']['adClickRateTotalDesc']})")
        competitionIntensity_lists.append(response['data']['competitionIntensity'])
        competitionIntensityDesc_lists.append(response['data']['competitionIntensityDesc'])
        adClickRateTotal_lists.append(response['data']['adClickRateStats']['adClickRateTotal'])
        adClickRateTotalDesc_lists.append(response['data']['adClickRateStats']['adClickRateTotalDesc'])
        sleep(GENERAL_REQUEST_WAIT_TIME)


def get_keyword_contents_competition_stats_for_itemscout():
    global blogCompetitionRatio_lists
    blogCompetitionRatio_lists = []  # ë¸”ë¡œê·¸ ê²½ìŸê°•ë„ ìˆ˜ì¹˜
    global blogCompetitionDesc_lists
    blogCompetitionDesc_lists = []  # ë¸”ë¡œê·¸ ê²½ìŸê°•ë„ ë“±ê¸‰
    global cafeCompetitionRatio
    cafeCompetitionRatio = []  # ì¹´í˜ ê²½ìŸê°•ë„ ìˆ˜ì¹˜
    global cafeCompetitionDesc_lists
    cafeCompetitionDesc_lists = []  # ì¹´í˜ ê²½ìŸê°•ë„ ë“±ê¸‰

    headers = {
        'accept': 'application/json, text/plain, */*',
        'origin': 'https://itemscout.io',
        'referer': 'https://itemscout.io/',
        'user-agent': fixed_user_agent,
    }

    for idx, keyword_id in enumerate(keyword_id_lists):
        response = requests.get(f'https://api.itemscout.io/api/v2/keyword/contents_competition_stats/{keyword_id}',
                                headers=headers).json()
        # pp(response)
        # print(response['data']['blogCompetitionRatio'])  # ë¸”ë¡œê·¸ ê²½ìŸê°•ë„ ìˆ˜ì¹˜
        # print(response['data']['blogCompetitionDesc'])  # ë¸”ë¡œê·¸ ê²½ìŸê°•ë„ ë“±ê¸‰
        # print(response['data']['cafeCompetitionRatio'])  # ì¹´í˜ ê²½ìŸê°•ë„ ìˆ˜ì¹˜
        # print(response['data']['cafeCompetitionDesc'])  # ì¹´í˜ ê²½ìŸê°•ë„ ë“±ê¸‰
        print(
            f"{idx + 1}. ë¸”ë¡œê·¸ ê²½ìŸê°•ë„({response['data']['blogCompetitionDesc']}) | ì¹´í˜ ê²½ìŸê°•ë„({response['data']['cafeCompetitionDesc']})")
        blogCompetitionRatio_lists.append(response['data']['blogCompetitionRatio'])
        blogCompetitionDesc_lists.append(response['data']['blogCompetitionDesc'])
        cafeCompetitionRatio.append(response['data']['cafeCompetitionRatio'])
        cafeCompetitionDesc_lists.append(response['data']['cafeCompetitionDesc'])
        sleep(GENERAL_REQUEST_WAIT_TIME)


def get_keyword_coupang_stats_for_itemscout():
    global coupangCompetitionRatio_lists
    coupangCompetitionRatio_lists = []  # ì¿ íŒ¡ ê²½ìŸê°•ë„ ìˆ˜ì¹˜
    global coupangCompetitionDesc_lists
    coupangCompetitionDesc_lists = []  # ì¿ íŒ¡ ê²½ìŸê°•ë„ ë“±ê¸‰

    headers = {
        'accept': 'application/json, text/plain, */*',
        'origin': 'https://itemscout.io',
        'referer': 'https://itemscout.io/',
        'user-agent': fixed_user_agent,
    }

    for idx, keyword_id in enumerate(keyword_id_lists):
        response = requests.get(f'https://api.itemscout.io/api/v2/keyword/coupang_stats/{keyword_id}',
                                headers=headers).json()
        # pp(response)
        # print(response['data']['coupangCompetitionRatio'])  # ì¿ íŒ¡ ê²½ìŸê°•ë„ ìˆ˜ì¹˜
        # print(response['data']['coupangCompetitionDesc'])  # ì¿ íŒ¡ ê²½ìŸê°•ë„ ë“±ê¸‰
        print(f"{idx + 1}. ì¿ íŒ¡ ê²½ìŸê°•ë„({response['data']['coupangCompetitionDesc']})")
        coupangCompetitionRatio_lists.append(response['data']['coupangCompetitionRatio'])
        coupangCompetitionDesc_lists.append(response['data']['coupangCompetitionDesc'])
        sleep(GENERAL_REQUEST_WAIT_TIME)


# def shorten_url(link_url):
#     shortener = pyshorteners.Shortener()

#     try:
#         shorten_url = shortener.isgd.short(link_url)  # https://is.gd
#         if shorten_url:
#             print(shorten_url)
#             shorten_url_lists.append(shorten_url)
#             return
#     except Exception as e:
#         print(f'Error: {e}')
#         pass

#     try:
#         shorten_url = shortener.tinyurl.short(link_url)  # https://tinyurl.com
#         if shorten_url:
#             print(shorten_url)
#             shorten_url_lists.append(shorten_url)
#             return
#     except Exception as e:
#         print(f'Error: {e}')
#         pass

#     try:
#         shorten_url = shortener.clckru.short(link_url)  # https://clck.ru
#         if shorten_url:
#             print(shorten_url)
#             shorten_url_lists.append(shorten_url)
#             return
#     except Exception as e:
#         print(f'Error: {e}')
#         pass

#     try:
#         shorten_url = shortener.dagd.short(link_url)  # https://da.gd
#         if shorten_url:
#             print(shorten_url)
#             shorten_url_lists.append(shorten_url)
#             return
#     except Exception as e:
#         print(f'Error: {e}')
#         pass

#     try:
#         shorten_url = shortener.osdb.short(link_url)  # http://osdb.link
#         if shorten_url:
#             print(shorten_url)
#             shorten_url_lists.append(shorten_url)
#             return
#     except Exception as e:
#         print(f'Error: {e}')
#         pass

#     try:
#         shorten_url = shortener.chilpit.short(link_url)  # http://chilp.it
#         if shorten_url:
#             print(shorten_url)
#             shorten_url_lists.append(shorten_url)
#             return
#     except Exception as e:
#         print(f'Error: {e}')
#         pass


def partner_coupang(keyword, _input_num=1):
    # if _input_num == '2':  # í‚¤ì›Œë“œ ê²€ìƒ‰ì´ ì•„ë‹ë•Œ
    #     print('\n' + C_BOLD + C_YELLOW + C_BGBLACK + '[íŒŒíŠ¸ë„ˆ ë§í¬ ìƒì„± ì‹œì‘]', C_END)

    global check_random_time

    target_url = 'https://www.coupang.com/np/search?component=&q=' + \
                 str(keyword) + '&channel=user'  # URL

    headers = {'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7,zh-TW;q=0.6,zh;q=0.5',
               'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36',
               'Accept-Encoding': 'gzip'
               }

    res = requests.get(url=target_url, headers=headers)
    soup = BeautifulSoup(res.text, "html.parser")

    descriptions_inner_lists = soup.select('div.descriptions-inner')
    image_lists = soup.select('dt.image')

    if len(descriptions_inner_lists) < 10:
        print('ê²€ìƒ‰ëœ ê²°ê³¼ê°€ ì—†ê±°ë‚˜ 10ê°œ ì´í•˜ì…ë‹ˆë‹¤. ë‹¤ì‹œê¸ˆ ì‹œë„í•˜ì„¸ìš”')
        check_random_time = 1
        return 0
    else:
        print('ê²€ìƒ‰ëœ ìƒí’ˆ ìˆ˜ëŸ‰ : ', len(descriptions_inner_lists))

    # # Top 10ê°œë§Œ ë¦¬ìŠ¤íŠ¸ì— ë‹´ê¸° (ì‚¬ìš©ë˜ëŠ” ë¦¬ìŠ¤íŠ¸ ì„¤ëª…)
    # product_name_lists = []  # ìƒí’ˆëª…
    # product_discount_rate_lists = []  # í• ì¸ë¥ ê³¼ ì›ë˜ê°€ê²©
    # product_price_lists = []  # ìƒí’ˆê°€ê²©
    # product_arrival_time_lists = []  # ë„ì°©ì˜ˆì •
    # product_rating_star_lists = []  # star í‰ê°€: ex.3.5
    # product_review_lists = []  # ìƒí’ˆë¦¬ë·° ìˆ˜
    # product_link_lists = []  # ìƒí’ˆ êµ¬ë§¤ ë§í¬
    # product_image_lists = []  # ìƒí’ˆ ì´ë¯¸ì§€

    for inner in descriptions_inner_lists[:10]:
        product_name = inner.select_one('div > div.name')  # ìƒí’ˆëª…
        if product_name is not None:
            # print(product_name.text)
            product_name_lists.append(product_name.text)
        else:
            product_name_lists.append('No data')
        product_discount_rate = inner.select_one('div.price-wrap > div.price > span.price-info')  # í• ì¸ë¥ ê³¼ ì›ë˜ê°€ê²©
        if product_discount_rate is not None:
            # print(product_discount_rate.text.lstrip())
            product_discount_rate_lists.append(f'{product_discount_rate.text.lstrip()}ì›')
        else:
            product_discount_rate_lists.append('')
        product_price = inner.select_one('div.price-wrap > div.price > em > strong')  # ìƒí’ˆê°€ê²©
        if product_price is not None:
            # print(product_price.text.replace(",", ""))
            product_price_lists.append(f"{product_price.text}ì›")
        else:
            product_price_lists.append('No data')
        product_arrival_time = inner.select_one('div.price-wrap > div.delivery > span.arrival-info')  # ë„ì°©ì˜ˆì •
        if product_arrival_time is not None:
           # print(product_arrival_time.text)
            product_arrival_time_lists.append("ìƒí’ˆí˜ì´ì§€ ì°¸ì¡°") #
        else:
            product_arrival_time_lists.append('No data')
        product_rating_star = inner.select_one(
            'div.other-info > div.rating-star > span.star > em.rating')  # star í‰ê°€: ex.3.5
        if product_rating_star is not None:
            # print(product_rating_star.text)
            product_rating_star_lists.append(product_rating_star.text)
        else:
            product_rating_star_lists.append('No data')
        product_review = inner.select_one('div.other-info > div > span.rating-total-count')  # ìƒí’ˆë¦¬ë·° ìˆ˜
        if product_review is not None:
            # print(re.sub("[()]", "", product_review.text))
            product_review_lists.append(re.sub("[()]", "", product_review.text))
        else:
            product_review_lists.append('0')

    product_links = soup.select('a.search-product-link')  # ìƒí’ˆ êµ¬ë§¤ ë§í¬
    for link in product_links[:10]:  # ìƒí’ˆ êµ¬ë§¤ ë§í¬ ë¦¬ìŠ¤íŠ¸ì— ë„£ê¸°
        p_link = "https://www.coupang.com" + link['href']
        product_link_lists.append(p_link)

    for image in image_lists[:10]:
        product_image = image.select_one('img.search-product-wrap-img')  # ìƒí’ˆ ì´ë¯¸ì§€
        # print(product_image)

        p_image = product_image.get('data-img-src')

        if p_image is None:
            p_image = product_image.get('src')
            p_image = f'https:{p_image}'
            # print(p_image)
            product_image_lists.append(p_image)
        else:
            p_image = f'https:{p_image}'
            # print(p_image)
            product_image_lists.append(p_image)


    # ì¶œë ¥
    count = 1
    for product_name, product_discount_rate, product_price, product_arrival_time, product_rating_star, product_review, product_link, product_image in zip \
                (product_name_lists, product_discount_rate_lists, product_price_lists, product_arrival_time_lists,
                 product_rating_star_lists,
                 product_review_lists, product_link_lists, product_image_lists):
        print(
            f'{count}. {product_name} | {product_discount_rate} | {product_price} | {product_arrival_time} | {product_rating_star} | {product_review} | \n{product_link} | \n{product_image}\n')
        count = count + 1

    # -----------------------------------------------------------------------
    # ì¿ íŒ¡ API ê°€ ì—†ëŠ” ì‚¬ëŒë“¤ì„ ìœ„í•œ ì¿ íŒ¡ ë§í¬ ìƒì„±
    # -----------------------------------------------------------------------

    def getPageKey(coupangurl):
        # URLì—ì„œ ìˆ«ì ë¶€ë¶„ì„ ì¶”ì¶œí•˜ëŠ” ì •ê·œí‘œí˜„ì‹
        pattern = r"/products/(\d+)"

        # ì •ê·œí‘œí˜„ì‹ì— ë§ëŠ” ë¶€ë¶„ ì°¾ê¸°
        match = re.search(pattern, coupangurl)

        pagekey = ''
        # ìˆ«ìë¥¼ ì¶”ì¶œí•œ ê²°ê³¼ ì¶œë ¥
        if match:
            extracted_number = match.group(1)
            pagekey = extracted_number
        else:
            pagekey = 0

        return pagekey

    def getProductType(coupangurl):
        product_type = 'AFFTDP'
        if '/vp/' in coupangurl:
            product_type = 'AFFSDP'

        return product_type

    def getQueryText(str, coupangurl):
        parsed_url = urlparse(coupangurl)
        query_params = parse_qs(parsed_url.query)

        value = query_params.get(str, [None])[0]
        return value

    def makeDirectPartnersLink(coupangurl):
        traceid = 'V0-153'
        coupang_product_type = getProductType(coupangurl)
        pagekey = getPageKey(coupangurl)

        itemId = getQueryText('itemId', coupangurl)
        vendorItemId = getQueryText('vendorItemId', coupangurl)

        channelid = CHANNELID

        p_url = f"https://link.coupang.com/re/{coupang_product_type}?lptag={AFCODE}&subid={channelid}&pageKey={pagekey}&traceid={traceid}&itemId={itemId}&vendorItemId={vendorItemId}"

        return p_url

    for original_link in product_link_lists[:10]:
        print(makeDirectPartnersLink(original_link))
        product_short_url_lists.append(makeDirectPartnersLink(original_link))

    # -----------------------------------------------------------------------
    # ì¿ íŒ¡íŒŒíŠ¸ë„ˆìŠ¤ API ë¥¼ ì‚¬ìš©í•œ ë§í¬ ìƒì„± ë°©ë²•
    # -----------------------------------------------------------------------

    # REQUEST_METHOD = "POST"
    # DOMAIN = "https://api-gateway.coupang.com"
    # URL = "/v2/providers/affiliate_open_api/apis/openapi/v1/deeplink"
    #
    # # Replace with your own ACCESS_KEY and SECRET_KEY
    # COOPANG_ACCESS_KEY = coopang_access_key
    # COOPANG_SECRET_KEY = coopang_secret_key
    #
    # print('ì¿ íŒ¡ íŒŒíŠ¸ë„ˆìŠ¤ API í•œë„(ì œí•œ) ë•Œë¬¸ì— ì¿ íŒ¡ ë§í¬ì˜ ë³€í™˜ì€ ë‹¤ì†Œ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í˜„ì¬ëŠ” 10ì´ˆ ê°„ê²©ìœ¼ë¡œ ìš”ì²­ì„ í•©ë‹ˆë‹¤.')
    # for idx, i in enumerate(product_link_lists[:10]):
    #     coupang_link = i  # ì¿ íŒ¡ë§í¬
    #     REQUEST = {"coupangUrls": [coupang_link]}  # í•´ë‹¹ ì¿ íŒ¡ë§í¬ ë°›ê¸°
    #
    #     def generateHmac(method, url, api_secret_key, api_access_key):
    #         path, *query = url.split('?')
    #         os.environ['TZ'] = 'GMT+0'
    #         dt_datetime = strftime('%y%m%d', gmtime()) + 'T' + \
    #                       strftime('%H%M%S', gmtime()) + 'Z'  # GMT+0
    #         msg = dt_datetime + method + path + (query[0] if query else '')
    #         signature = hmac.new(bytes(api_secret_key, 'utf-8'),
    #                              msg.encode('utf-8'), hashlib.sha256).hexdigest()
    #
    #         return 'CEA algorithm=HmacSHA256, access-key={}, signed-date={}, signature={}'.format(api_access_key,
    #                                                                                               dt_datetime,
    #                                                                                               signature)
    #
    #     authorization = generateHmac(
    #         REQUEST_METHOD, URL, COOPANG_SECRET_KEY, COOPANG_ACCESS_KEY)
    #     url = "{}{}".format(DOMAIN, URL)
    #     response = requests.request(method=REQUEST_METHOD, url=url,
    #                                 headers={
    #                                     "Authorization": authorization,
    #                                     "Content-Type": "application/json"
    #                                 },
    #                                 data=json.dumps(REQUEST)
    #                                 )
    #
    #     # print(response.json())  #í™•ì¸
    #     if idx != 0:
    #         time.sleep(5)  # 10ì´ˆë§ˆë‹¤ í•œë²ˆì”© (ì¿ íŒ¡ ì œì•½ ë•Œë¬¸)
    #         # - ê²€ìƒ‰ API: 1ì‹œê°„ë‹¹ 10íšŒ
    #         # - ë¦¬í¬íŠ¸ API: 1ì‹œê°„ë‹¹ 50íšŒ
    #         # - ëª¨ë“  API: 1ë¶„ë‹¹ 100íšŒ
    #         # - íŒŒíŠ¸ë„ˆìŠ¤ ì›¹ì˜ ë§í¬ìƒì„± ê¸°ëŠ¥: 1ë¶„ë‹¹ 50íšŒ44
    #
    #     text = response.json()
    #     # print(text)
    #     try:
    #         text_2 = text['data']
    #         print(f'{idx + 1}. {text_2}')
    #     except:
    #         # ì—†ì„ì‹œ ê°€ì§œ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    #         text_2 = ['https://www.coupang.com/np/coupangglobal']
    #         # print('except(1) - ê°€ì§œ ë§í¬ ìƒì„±')
    #     for i in text_2:
    #         try:
    #             product_short_url = i['shortenUrl']
    #         except:
    #             product_short_url = fake_coopang_link  # ê°€ì§œ ë§í¬ ì§‘ì–´ë„£ê¸°
    #             # print('except(2) - ê°€ì§œ ë§í¬ ìƒì„±')
    #         print(product_short_url)  # í™•ì¸
    #         product_short_url_lists.append(product_short_url)
    #
    # print("\nìµœì¢… ì¿ íŒ¡ íŒŒíŠ¸ë„ˆìŠ¤ ìˆ ë§í¬ê°€ ìƒì„± ë˜ì—ˆìŠµë‹ˆë‹¤.")  # ìµœì¢… í™•ì¸

    if len(product_image_lists) < 10:
        check_random_time = 1
        print('product_image_lists, out of range')
        return 0


def make_md_file(input_num, keyword, age='10,10', gender='f,m'):
    # ê¸€ ì œëª© êµ¬ì„±
    if input_num == '1':  # ê²€ìƒ‰ì„ í†µí•œ ë°œí–‰ì´ë¼ë©´,
        post_title = product_name_lists[0][:69]
    else:  # ê²€ìƒ‰ì„ í†µí•œ ë°œí–‰ì´ ì•„ë‹ˆë¼ datalab ì— ì˜í•œ ëœë¤í•œ ë‚´ìš©ì´ë¼ë©´,
        if gender == 'f,m':
            if age == '10,10':
                post_title = '[10ëŒ€][ì „ì—°ë ¹] ' + product_name_lists[0][:69]
            elif age == '20,20':
                post_title = '[20ëŒ€][ì „ì—°ë ¹] ' + product_name_lists[0][:69]
            elif age == '30,30':
                post_title = '[30ëŒ€][ì „ì—°ë ¹] ' + product_name_lists[0][:69]
            elif age == '40,40':
                post_title = '[40ëŒ€][ì „ì—°ë ¹] ' + product_name_lists[0][:69]
            elif age == '50,50':
                post_title = '[50ëŒ€][ì „ì—°ë ¹] ' + product_name_lists[0][:69]
            elif age == '60,60':
                post_title = '[60ëŒ€][ì „ì—°ë ¹] ' + product_name_lists[0][:69]
            else:
                post_title = f'[{str(age.split(",")[0])}~{str(age.split(",")[1])}ëŒ€] ' + \
                             product_name_lists[0][:69]
        elif gender == 'f':
            if age == '10,10':
                post_title = '[10ëŒ€][ì—¬ì„±] ' + product_name_lists[0][:69]
            elif age == '20,20':
                post_title = '[20ëŒ€][ì—¬ì„±] ' + product_name_lists[0][:69]
            elif age == '30,30':
                post_title = '[30ëŒ€][ì—¬ì„±] ' + product_name_lists[0][:69]
            elif age == '40,40':
                post_title = '[40ëŒ€][ì—¬ì„±] ' + product_name_lists[0][:69]
            elif age == '50,50':
                post_title = '[50ëŒ€][ì—¬ì„±] ' + product_name_lists[0][:69]
            elif age == '60,60':
                post_title = '[60ëŒ€][ì—¬ì„±] ' + product_name_lists[0][:69]
            else:
                post_title = f'[{str(age.split(",")[0])}~{str(age.split(",")[1])}ëŒ€][ì—¬ì„±] ' + \
                             product_name_lists[0][:69]
        else:
            if age == '10,10':
                post_title = '[10ëŒ€][ë‚¨ì„±] ' + product_name_lists[0][:69]
            elif age == '20,20':
                post_title = '[20ëŒ€][ë‚¨ì„±] ' + product_name_lists[0][:69]
            elif age == '30,30':
                post_title = '[30ëŒ€][ë‚¨ì„±] ' + product_name_lists[0][:69]
            elif age == '40,40':
                post_title = '[40ëŒ€][ë‚¨ì„±] ' + product_name_lists[0][:69]
            elif age == '50,50':
                post_title = '[50ëŒ€][ë‚¨ì„±] ' + product_name_lists[0][:69]
            elif age == '60,60':
                post_title = '[60ëŒ€][ë‚¨ì„±] ' + product_name_lists[0][:69]
            else:
                post_title = f'[{str(age.split(",")[0])}~{str(age.split(",")[1])}ëŒ€][ë‚¨ì„±] ' + \
                             product_name_lists[0][:69]

    # HTML content
    partner_announcement = 'https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fb8pNGP%2FbtrH0SmcF5j%2FoLshlkyBZGVAOxyYqnEvOK%2Fimg.jpg'
    naver_datalab_url = 'https://itemscout.io/'

    post_head = f'''---
title: "{keyword} TOP10 ì¶”ì²œ - {post_title}"
author: Jirm Shin
categories: shopping
tags: [Top10, shopping]
pin: true
---

í•´ë‹¹ ê²Œì‹œë¬¼ì—ì„œëŠ” [**ë¶„ì„ë„êµ¬**]({naver_datalab_url})ë¥¼ ì´ìš©í•˜ì—¬ ì„±ë³„, ì—°ë ¹ë³„ ë“±ì˜ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ [**ìƒí’ˆ**]({fake_coopang_link})ë“¤ì„ ì¶”ì²œí•´ë“œë¦¬ê³  ìˆìŠµë‹ˆë‹¤.
'''

    post_body = ''
    for item_index in range(len(product_short_url_lists)):
        if (item_index + 1) % 2 == 1:
            align1 = 'left'
            align2 = 'right'
        else:
            align1 = 'right'
            align2 = 'left'
        if product_rating_star_lists[item_index] == 'No data':
            star = '1'
            star_image_link = 'https://user-images.githubusercontent.com/78655692/151471925-e5f35751-d4b9-416b-b41d-a059267a09e3.png'
        else:
            if int(float(product_rating_star_lists[item_index])) <= 2:
                star = '1'
                star_image_link = 'https://user-images.githubusercontent.com/78655692/151471925-e5f35751-d4b9-416b-b41d-a059267a09e3.png'
            elif int(float(product_rating_star_lists[item_index])) <= 4:
                star = '2'
                star_image_link = 'https://user-images.githubusercontent.com/78655692/151471960-29c5febe-c509-4c6d-99f4-a2203eb193c5.png'
            else:
                star = '3'
                star_image_link = 'https://user-images.githubusercontent.com/78655692/151471989-9e21d7a8-a7b6-44b0-b598-2bb204b56b00.png'
        post_body = post_body + f"""
### [{item_index + 1}] {keyword} íŒë§¤ ìˆœìœ„ <img width="81" alt="star{star}" src="{star_image_link}">

![{keyword} TOP01]({product_image_lists[item_index]}){{: width="300" height="300" .w-50 .{align1}}}


[{product_name_lists[item_index]}]({product_short_url_lists[item_index]})
<br>
- í• ì¸ìœ¨ê³¼ ì›ë˜ê°€ê²©: {product_discount_rate_lists[item_index]}
- ê°€ê²©: {product_price_lists[item_index]}
- ë„ì°©ì˜ˆì •: {product_arrival_time_lists[item_index]}
- star í‰ê°€: {product_rating_star_lists[item_index]}
- ë¦¬ë·°ìˆ˜: {product_review_lists[item_index]}
<br>
<br>
[**[CLICK]**]({product_short_url_lists[item_index]}){{: .{align2}}}
<br>
<br>

---
"""

    post_partner_content = f"<br><br><br><br><br> [ğŸ’¦ ğŸ’¦ ğŸ’¦ íŒŒíŠ¸ë„ˆìŠ¤ í™œë™ì„ í†µí•´ ì¼ì •ì•¡ì˜ ìˆ˜ìˆ˜ë£Œë¥¼ ì œê³µë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤]({fake_coopang_link}){{: .right}}"

    post_content = post_head + post_body + post_partner_content

    # í˜„ì¬ ë‚ ì§œë¥¼ ì´ìš©í•´ íŒŒì¼ëª… ìƒì„±
    yesterday = datetime.datetime.now() - timedelta(days=1)
    timestring = yesterday.strftime('%Y-%m-%d')
    # yesterday = datetime.now() - timedelta(days=1)
    # timestring = yesterday.strftime('%Y-%m-%d')

    # íŒŒì¼ëª… ìƒì„±
    product_name_result = re.sub(r"[^\uAC00-\uD7A30-9a-zA-Z\s]", "", product_name_lists[0])  # íŠ¹ìˆ˜ë¬¸ì ì œê±°
    filename = f"{timestring}-{keyword}-{product_name_result.replace(' ', '-')}.md"

    # íŒŒì¼ ê²½ë¡œ ì„¤ì •
    blog_directory = post_md_location
    filepath = os.path.join(blog_directory, filename)

    # íŒŒì¼ì— ë¸”ë¡œê·¸ ë‚´ìš© ì‘ì„±
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(post_content)
        f.close()


# main start
if __name__ == '__main__':
    try:
        start_time = time.time()  # ì‹œì‘ ì‹œê°„ ì²´í¬
        now = datetime.datetime.now()
        print("START TIME : ", now.strftime('%Y-%m-%d %H:%M:%S'))
        print("\nSTART...")

        # print('\n' + C_BOLD + C_YELLOW + C_BGBLACK + '[í¬ë¡¬ ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì‹œì‘]', C_END)
        # driver = init_driver()
        # sleep(PAUSE_TIME)
        # print('\n' + C_BOLD + C_YELLOW + C_BGBLACK + '[í¬ë¡¬ ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì™„ë£Œ]', C_END)
        #
        # print('\n' + C_BOLD + C_YELLOW + C_BGBLACK + '[ì•„ì´í…œ ìŠ¤ì¹´ìš°íŠ¸ ë¡œê·¸ì¸ ì‹œì‘(3ë¶„ì•ˆì— ë¡œê·¸ì¸ì„ í•´ì£¼ì„¸ìš”)]', C_END)
        # itemscout_login(driver)
        # sleep(PAUSE_TIME)
        # print('\n' + C_BOLD + C_YELLOW + C_BGBLACK + '[ì•„ì´í…œ ìŠ¤ì¹´ìš°íŠ¸ ë¡œê·¸ì¸ ì™„ë£Œ]', C_END)
        #
        # print('\n' + C_BOLD + C_YELLOW + C_BGBLACK + '[ì•„ì´í…œ ìŠ¤ì¹´ìš°íŠ¸ ë¡œê·¸ì¸ í›„ ì¿ í‚¤ê°’ ì €ì¥ ë° ì„¸ì…˜ ë¦¬í„´ ì‹œì‘]', C_END)
        # itemscout_session = get_cookies_session(driver, 'https://portals.aliexpress.com/affiportals/web/link_generator.htm')
        # sleep(PAUSE_TIME)
        # print('\n' + C_BOLD + C_YELLOW + C_BGBLACK + '[ì•„ì´í…œ ìŠ¤ì¹´ìš°íŠ¸ ë¡œê·¸ì¸ í›„ ì¿ í‚¤ê°’ ì €ì¥ ë° ì„¸ì…˜ ë¦¬í„´ ì™„ë£Œ]', C_END)

        while True:
            # partner info
            product_link_lists = []
            product_image_lists = []
            product_name_lists = []
            product_price_lists = []
            product_short_url_lists = []
            shorten_url_lists = []

            print('ê²€ìƒ‰ì„ í†µí•´ íŒŒíŠ¸ë„ˆ urlì–»ê¸° ì›í•˜ì‹œë©´' + C_BOLD +
                  C_RED + '(1)' + C_END + 'ë¥¼ ëˆŒëŸ¬ì£¼ì‹œê³ ,')
            print('ì•„ì´í…œìŠ¤ì¹´ìš°íŠ¸ì—ì„œ ì¶”ì²œí•˜ëŠ” ìƒí’ˆì— ëŒ€í•œ íŒŒíŠ¸ë„ˆ urlì„ ì–»ê¸° ì›í•˜ì‹ ë‹¤ë©´' +
                  C_BOLD + C_RED + '(2)' + C_END + 'ë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”')
            print('í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•˜ê³  ì‹¶ìœ¼ë©´' + C_BOLD +
                  C_RED + '(q)' + C_END + 'ë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”')
            input_num = input('ì›í•˜ëŠ” ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš” : ')

            # input_num = '2'

            if input_num == 'q':
                break

            if input_num == '1':
                print('ì…ë ¥í•˜ì‹  í‚¤ëŠ” 1 ì…ë‹ˆë‹¤...\n')
                sleep(3)
                query = input('ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš” : ')
                query = query.replace(' ', '+')

                print('\n' + C_BOLD + C_YELLOW +
                      C_BGBLACK + '[íŒŒíŠ¸ë„ˆ ë§í¬ ìƒì„± ì‹œì‘]', C_END)
                ret = partner_coupang(query, input_num)
                if ret == 0:
                    continue
                print('\n' + C_BOLD + C_YELLOW +
                      C_BGBLACK + '[íŒŒíŠ¸ë„ˆ ë§í¬ ìƒì„± ì™„ë£Œ]', C_END)

                # print('\n' + C_BOLD + C_YELLOW + C_BGBLACK +
                #       '[íŒŒíŠ¸ë„ˆ ë‹¨ì¶• url ìƒì„± ì‹œì‘]', C_END)
                # for i in range(0, len(product_short_url_lists)):
                #     shorten_url(product_short_url_lists[i])
                # print('\n' + C_BOLD + C_YELLOW + C_BGBLACK +
                #       '[íŒŒíŠ¸ë„ˆ ë‹¨ì¶• url ìƒì„± ì™„ë£Œ]', C_END)

                print('\n' + C_BOLD + C_YELLOW + C_BGBLACK +
                      '[md(mark down) íŒŒì¼ ìƒì„± ì‹œì‘]', C_END)
                make_md_file(input_num, query)
                print('\n' + C_BOLD + C_YELLOW + C_BGBLACK +
                      '[md(mark down) íŒŒì¼ ìƒì„± ì™„ë£Œ]', C_END)

            elif input_num == '2':
                print('ì…ë ¥í•˜ì‹  í‚¤ëŠ” 2 ì…ë‹ˆë‹¤...\n')
                sleep(2)

                # global random_itemscout_category_lists
                # global random_itemscout_cid_lists

                # ì•„ì´í…œìŠ¤ì¹´ìš°íŠ¸ ì¹´í…Œê³ ë¦¬ (1ì°¨ ë¶„ë¥˜ë§Œ ìˆì„ë•Œ)
                random_itemscout_cid_lists = ['1', '2', '3', '4', '5',
                                              '6', '7', '8', '9', '10', '11', '45830']

                random_number_list = []
                recursive_random_int_no_again(len(random_itemscout_cid_lists))
                print(f'random_number_list >> {random_number_list}')
                count = 1
                for number in random_number_list:
                    print('\n' + C_BOLD + C_YELLOW +
                          C_BGBLACK + 'count : ', count, C_END)
                    print('number of al_list = [' + str(number) + ']')
                    cid = random_itemscout_cid_lists[number - 1]
                    # ì•„ì´í…œìŠ¤ì¹´ìš°íŠ¸ ê¸°ê°„
                    itemscout_duration = '30d'  # (duration 30d ë˜ëŠ” ë‚ ì§œ ì„¤ì •, duration: 2023-03,2023-04 3ì›”ë¶€í„° 4ì›”)
                    # ì•„ì´í…œìŠ¤ì¹´ìš°íŠ¸ ë‚˜ì´
                    random_age_lists = ['10,10', '20,20', '30,30', '40,40', '50,50', '60,60', '10,20', '10,30', '10,40',
                                        '10,50', '10,60', '20,30', '20,40', '20,50', '20,60'
                                        , '30,40', '30,50', '30,60', '40,50', '40,60', '50,60']
                    x = random.randint(0, len(random_age_lists) - 1)
                    itemscout_ages = random_age_lists[x]
                    # ì•„ì´í…œìŠ¤ì¹´ìš°íŠ¸ ì„±ë³„
                    random_gender_lists = ['f,m', 'f', 'm']  # ì „ì²´, ì—¬ì„±, ë‚¨ì„±
                    x = random.randint(0, len(random_gender_lists) - 1)
                    itemscout_gender = random_gender_lists[x]

                    # ìˆ˜ë™ìœ¼ë¡œ ì•Œì•„ë³´ë ¤ë©´ ì•„ë˜ì˜ ì½”ë“œ í™œìš©
                    # # ì•„ì´í…œìŠ¤ì¹´ìš°íŠ¸ ì„±ë³„
                    # itemscout_gender = 'f,m'  # (sample. ë‚¨ì„±ê³¼ ì—¬ì„±ì´ë©´ f,m)
                    # # ì•„ì´í…œìŠ¤ì¹´ìš°íŠ¸ ë‚˜ì´
                    # itemscout_ages = '10,60'  # (sample. 20,60 ì´ë©´ 20ëŒ€ë¶€í„° 60ëŒ€ê¹Œì§€)

                    print(
                        f'{C_BOLD}{C_YELLOW}{C_BGBLACK}RANDOM info >>> cid {cid} | duration {itemscout_duration} | age {itemscout_ages} | gender {itemscout_gender} {C_END}')

                    print(f'\n{C_BOLD}{C_YELLOW}{C_BGBLACK}[itemscout ì—ì„œ ì•„ì´í…œë“¤ ë¦¬ìŠ¤íŠ¸ë¥¼ ì „ì²´ ë°›ê¸° ì‹œì‘]{C_END}')
                    get_items_for_itemscout(cid, random_itemscout_category_lists[number - 1], itemscout_duration,
                                            itemscout_ages, itemscout_gender)
                    sleep(PAUSE_TIME)
                    print(f'\n{C_BOLD}{C_YELLOW}{C_BGBLACK}[itemscout ì—ì„œ ì•„ì´í…œë“¤ ë¦¬ìŠ¤íŠ¸ë¥¼ ì „ì²´ ë°›ê¸° ì™„ë£Œ]{C_END}')

                    # print(f'\n{C_BOLD}{C_YELLOW}{C_BGBLACK}[itemscout ì—ì„œ ê° keyword ì— ë”°ë¥¸ ì „ë°˜ì ì¸ ë¶„ì„ ì‹œì‘]{C_END}')
                    # get_keyword_stats_for_itemscout()
                    # sleep(PAUSE_TIME)
                    # print(f'\n{C_BOLD}{C_YELLOW}{C_BGBLACK}[itemscout ì—ì„œ ê° keyword ì— ë”°ë¥¸ ì „ë°˜ì ì¸ ë¶„ì„ ì™„ë£Œ]{C_END}')
                    # #
                    # print(f'\n{C_BOLD}{C_YELLOW}{C_BGBLACK}[itemscout ì—ì„œ ê° keyword ì— ë”°ë¥¸ ë¸”ë¡œê·¸ ì¹´í˜ ë¶„ì„ ì‹œì‘]{C_END}')
                    # get_keyword_contents_competition_stats_for_itemscout()
                    # sleep(PAUSE_TIME)
                    # print(f'\n{C_BOLD}{C_YELLOW}{C_BGBLACK}[itemscout ì—ì„œ ê° keyword ì— ë”°ë¥¸ ë¸”ë¡œê·¸ ì¹´í˜ ë¶„ì„ ì™„ë£Œ]{C_END}')

                    # print(f'\n{C_BOLD}{C_YELLOW}{C_BGBLACK}[itemscout ì—ì„œ ê° keyword ì— ë”°ë¥¸ ì¿ íŒ¡ ë¶„ì„ ì‹œì‘]{C_END}')
                    # get_keyword_coupang_stats_for_itemscout()
                    # sleep(PAUSE_TIME)
                    # print(f'\n{C_BOLD}{C_YELLOW}{C_BGBLACK}[itemscout ì—ì„œ ê° keyword ì— ë”°ë¥¸ ì¿ íŒ¡ ë¶„ì„ ì™„ë£Œ]{C_END}')

                    # count = 0  # ì²˜ìŒì—ëŠ” ëŒ€ê¸° ì‹œê°„ì„ ê°€ì§€ì§€ ì•Šë„ë¡ í•¨
                    print("í‚¤ì›Œë“œ ë°ì´í„° >>> ", keyword_name_lists)
                    for j in range(0, len(keyword_name_lists)):
                        product_link_lists = []
                        product_image_lists = []
                        product_name_lists = []
                        product_price_lists = []
                        product_short_url_lists = []
                        shorten_url_lists = []
                        # sleep(LONG_PAUSE_TIME)
                        if j == 0 or check_random_time == 1:
                            now = datetime.datetime.now()
                            current_hour = now.strftime('%H')
                            today_date = str(datetime.datetime.now())
                            today_date = today_date[:today_date.rfind(
                                ':')].replace('-', '.')
                            print('\n')
                            print(today_date)
                            check_random_time = 0
                            pass
                        else:
                            # random_start_time = random.randint(5400, 7200)  # ê° ë°œí–‰ì„ random í•˜ê²Œ ì‹œì‘ (90ë¶„~120ë¶„ ì‚¬ì´)
                            # random_start_time = random.randint(3600, 3800)  # ê° ë°œí–‰ì„ random í•˜ê²Œ ì‹œì‘ (90ë¶„~120ë¶„ ì‚¬ì´)
                            # random_start_time = random.randint(300, 400)
                            random_start_time = random.randint(120, 300)
                            now = datetime.datetime.now()
                            current_hour = now.strftime('%H')
                            today_date = str(datetime.datetime.now())
                            today_date = today_date[:today_date.rfind(
                                ':')].replace('-', '.')
                            print('\n')
                            print('í˜„ì¬ ì‹œê°„: ', today_date)
                            print('ë‹¤ìŒ ì‹œì‘ ì‹œê°„: ', strftime(
                                "%H:%M:%S", gmtime(random_start_time)))
                            # print('random start time = ', random_start_time)
                            sleep(random_start_time)

                            check_random_time = 0

                        print('\n' + C_BOLD + C_YELLOW +
                              C_BGBLACK + '[íŒŒíŠ¸ë„ˆ ë§í¬ ìƒì„± ì‹œì‘]', C_END)
                        print(
                            f'{C_BOLD} {C_YELLOW} {C_BGBLACK} íŒŒíŠ¸ë„ˆ ê²€ìƒ‰ ìƒí’ˆ : [{x}] {keyword_name_lists[x - 1]} {C_END}')
                        ret = partner_coupang(keyword_name_lists[j], input_num)
                        if ret == 0:
                            continue
                        print('\n' + C_BOLD + C_YELLOW +
                              C_BGBLACK + '[íŒŒíŠ¸ë„ˆ ë§í¬ ìƒì„± ì™„ë£Œ]', C_END)

                        # print('\n' + C_BOLD + C_YELLOW +
                        #       C_BGBLACK + '[ë‹¨ì¶• url ìƒì„± ì‹œì‘]', C_END)
                        # for t in range(0, len(product_short_url_lists)):
                        #     shorten_url(product_short_url_lists[t])
                        # print('\n' + C_BOLD + C_YELLOW +
                        #       C_BGBLACK + '[ë‹¨ì¶• url ìƒì„± ì™„ë£Œ]', C_END)

                        print('\n' + C_BOLD + C_YELLOW + C_BGBLACK +
                              '[md(mark down) íŒŒì¼ ìƒì„± ì‹œì‘]', C_END)
                        make_md_file(input_num, keyword_name_lists[j], itemscout_ages, itemscout_gender)
                        print('\n' + C_BOLD + C_YELLOW + C_BGBLACK +
                              '[md(mark down) íŒŒì¼ ìƒì„± ì™„ë£Œ]', C_END)

                count = count + 1

            else:
                print("ì˜ëª» ì…ë ¥ í•˜ì˜€ìŠµë‹ˆë‹¤. 1, 2, q ì¤‘ì—ì„œ ì„ íƒí•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.")
                continue

    finally:
        # driver.close() # ë§ˆì§€ë§‰ ì°½ì„ ë‹«ê¸° ìœ„í•´ì„œëŠ” í•´ë‹¹ ì£¼ì„ ì œê±°
        # driver.quit()
        end_time = time.time()  # ì¢…ë£Œ ì‹œê°„ ì²´í¬
        ctime = end_time - start_time
        time_list = str(datetime.timedelta(seconds=ctime)).split(".")
        print("\nì‹¤í–‰ì‹œê°„(ì´ˆ)", ctime)
        print("ì‹¤í–‰ì‹œê°„ (ì‹œ:ë¶„:ì´ˆ)", time_list)
        now = datetime.datetime.now()
        print("END TIME : ", now.strftime('%Y-%m-%d %H:%M:%S'))
        print("\nEND...")
